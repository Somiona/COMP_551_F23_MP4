{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AlexNet In Pytorch"
      ],
      "metadata": {
        "id": "JGbyhU_S9bIj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-lPYexe9Yek"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision, os\n",
        "\n",
        "def train_alexnet(num_steps=100000):\n",
        "    print(\"Making alexnet...\")\n",
        "    alexnet = make_untrained_alexnet()\n",
        "    alexnet.train()\n",
        "    print(\"Loading datasets...\")\n",
        "    train_loader, val_loader = get_train_and_val_data_loaders()\n",
        "    print(\"Training classifier...\")\n",
        "    checkpointer = make_checkpointing_function(val_loader, checkpoint_dir='checkpoints')\n",
        "    train_classifier(alexnet, train_loader,\n",
        "                     max_iter=num_steps,\n",
        "                     momentum=0.9,\n",
        "                     init_lr=2e-2,\n",
        "                     weight_decay=5e-4,\n",
        "                     monitor=checkpointer)\n",
        "    return alexnet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from collections import OrderedDict\n",
        "def make_untrained_alexnet():\n",
        "    # channel widths\n",
        "    w = [3, 96, 256, 384, 384, 256, 4096, 4096, 365]\n",
        "    # Alexnet splits channels into groups\n",
        "    groups = [1, 2, 1, 2, 2]\n",
        "    model = nn.Sequential(OrderedDict([\n",
        "        ('conv1', nn.Conv2d(w[0], w[1], kernel_size=11,\n",
        "            stride=4,\n",
        "            groups=groups[0], bias=True)),\n",
        "        ('relu1', nn.ReLU(inplace=True)),\n",
        "        ('pool1', nn.MaxPool2d(kernel_size=3, stride=2)),\n",
        "        ('conv2', nn.Conv2d(w[1], w[2], kernel_size=5, padding=2,\n",
        "            groups=groups[1], bias=True)),\n",
        "        ('relu2', nn.ReLU(inplace=True)),\n",
        "        ('pool2', nn.MaxPool2d(kernel_size=3, stride=2)),\n",
        "        ('conv3', nn.Conv2d(w[2], w[3], kernel_size=3, padding=1,\n",
        "            groups=groups[2], bias=True)),\n",
        "        ('relu3', nn.ReLU(inplace=True)),\n",
        "        ('conv4', nn.Conv2d(w[3], w[4], kernel_size=3, padding=1,\n",
        "            groups=groups[3], bias=True)),\n",
        "        ('relu4', nn.ReLU(inplace=True)),\n",
        "        ('conv5', nn.Conv2d(w[4], w[5], kernel_size=3, padding=1,\n",
        "            groups=groups[4], bias=True)),\n",
        "        ('relu5', nn.ReLU(inplace=True)),\n",
        "        ('pool5', nn.MaxPool2d(kernel_size=3, stride=2)),\n",
        "        ('flatten', nn.Flatten()),\n",
        "        ('fc6', nn.Linear(w[5] * 6 * 6, w[6], bias=True)),\n",
        "        ('relu6', nn.ReLU(inplace=True)),\n",
        "        ('dropout6', nn.Dropout()),\n",
        "        ('fc7', nn.Linear(w[6], w[7], bias=True)),\n",
        "        ('relu7', nn.ReLU(inplace=True)),\n",
        "        ('dropout7', nn.Dropout()),\n",
        "        ('fc8', nn.Linear(w[7], w[8]))\n",
        "    ]))\n",
        "    # Setup the initial parameters randomly\n",
        "    for n, p in model.named_parameters():\n",
        "        if 'bias' in n:\n",
        "            torch.nn.init.zeros_(p)\n",
        "        else:\n",
        "            torch.nn.init.kaiming_normal_(p, nonlinearity='relu')\n",
        "    model.cuda() #no cuda on colab\n",
        "    model.train()\n",
        "    return model"
      ],
      "metadata": {
        "id": "CAC3-R1FyAiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display network"
      ],
      "metadata": {
        "id": "rDOpBprdyG-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = make_untrained_alexnet()\n",
        "for n, p in a.named_parameters():\n",
        "    print(n, tuple(p.shape))"
      ],
      "metadata": {
        "id": "uUgfrnAMyBk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "PPJUiQBVyXUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_places_data_set(split, crop_size=227, download=True):\n",
        "    transform=torchvision.transforms.Compose([\n",
        "               torchvision.transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
        "               torchvision.transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
        "                ])\n",
        "    if split == 'train':\n",
        "      return torchvision.datasets.MNIST(\n",
        "          root='./data',\n",
        "          train=True,\n",
        "          download=True,\n",
        "          transform=transform\n",
        "      )\n",
        "    return torchvision.datasets.MNIST(\n",
        "          root='./data',\n",
        "          train=False,\n",
        "          download=True,\n",
        "          transform=transform\n",
        "      )\n",
        "def get_train_and_val_data_loaders():\n",
        "    return [\n",
        "        torch.utils.data.DataLoader(\n",
        "            get_places_data_set(split),\n",
        "            batch_size=32, shuffle=(split == 'train'))\n",
        "        for split in ['train', 'val']\n",
        "    ]"
      ],
      "metadata": {
        "id": "-tST4zhMyYta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "BodOUgqIyGN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(model, train_data_loader, max_iter,\n",
        "                     momentum=0.9, init_lr=2e-2, weight_decay=5e-4,\n",
        "                     monitor=None):\n",
        "    if monitor is not None:\n",
        "        monitor(model, 0, 0.0, 0.0, 0)\n",
        "\n",
        "    optimizer = torch.optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=init_lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, init_lr, max_iter)\n",
        "    iter_num = 0\n",
        "\n",
        "\n",
        "    while iter_num < max_iter:\n",
        "        for t_input, t_target in train_data_loader:\n",
        "            # Copy data into the gpu\n",
        "            input_var, target_var = [d.cuda() for d in [t_input, t_target]]\n",
        "            # Evaluate model\n",
        "            output = model(input_var)\n",
        "            loss = torch.nn.functional.cross_entropy(output, target_var)\n",
        "            # Perform one step of SGD\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step() # Learning rate schedule\n",
        "            # Check training set accuracy\n",
        "            _, pred = output.max(1)\n",
        "            batch_size = len(t_input)\n",
        "            accuracy = target_var.detach().eq(pred).float().sum().item() / batch_size\n",
        "            # Advance, and print out some stats\n",
        "            iter_num += 1\n",
        "\n",
        "            print(1)\n",
        "            if monitor is not None:\n",
        "                monitor(model, iter_num, loss, accuracy, batch_size)\n",
        "            if iter_num >= max_iter:\n",
        "                break"
      ],
      "metadata": {
        "id": "dIMtl2lEyDbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "3NTapSXmydjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_val_accuracy_and_loss(model, val_data_loader):\n",
        "    '''\n",
        "    Evaluates the model (in inference mode) on holdout data.\n",
        "    '''\n",
        "    model.eval()\n",
        "    val_loss, val_acc = AverageMeter(), AverageMeter()\n",
        "    for input, target in val_data_loader:\n",
        "        input_var, target_var = [d.cuda() for d in [input, target]]\n",
        "        with torch.no_grad():\n",
        "            output = model(input_var)\n",
        "            loss = torch.nn.functional.cross_entropy(output, target_var)\n",
        "            _, pred = output.max(1)\n",
        "            accuracy = (target_var.eq(pred)\n",
        "                    ).data.float().sum().item() / input.size(0)\n",
        "        val_acc.update(accuracy, input.size(0))\n",
        "        val_loss.update(loss.data.item(), input.size(0))\n",
        "    return val_acc, val_loss\n",
        "\n",
        "def save_model_iteration(model, iter_num, checkpoint_dir):\n",
        "    '''\n",
        "    Saves the current parameters of the model to a file.\n",
        "    '''\n",
        "    torch.save(model.state_dict(), os.path.join(checkpoint_dir, f'iter_{iter_num}.pth'))\n",
        "\n",
        "def make_checkpointing_function(val_data_loader, checkpoint_dir=None, checkpoint_freq=100):\n",
        "    '''\n",
        "    Makes a callback to monitor training and make checkpoints.\n",
        "    '''\n",
        "    avg_train_accuracy, avg_train_loss = AverageMeter(), AverageMeter()\n",
        "    def monitor(model, iter_num, loss, accuracy, batch_size):\n",
        "        avg_train_accuracy.update(accuracy, batch_size)\n",
        "        avg_train_loss.update(loss, batch_size)\n",
        "        if iter_num % checkpoint_freq == 0:\n",
        "            val_accuracy, val_loss = measure_val_accuracy_and_loss(model, val_data_loader)\n",
        "            if checkpoint_dir is not None:\n",
        "                save_model_iteration(model, iter_num, checkpoint_dir)\n",
        "            print(f'Iter {iter_num}, ' +\n",
        "                  f'train acc {avg_train_accuracy.avg:.3g} loss {avg_train_loss.avg:.3g}, ' +\n",
        "                  f'val acc {val_accuracy.avg:.3g}, loss {val_loss.avg:.3g}')\n",
        "            model.train()\n",
        "    return monitor\n",
        "\n",
        "class AverageMeter(object):\n",
        "    '''\n",
        "    To keep running averages.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val = 0.\n",
        "        self.avg = 0.\n",
        "        self.sum = 0.\n",
        "        self.count = 0\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        if self.count:\n",
        "            self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "fNLLlYVGycgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations = 100\n",
        "train_alexnet(num_iterations)"
      ],
      "metadata": {
        "id": "f48aMUcPyjUK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}